

## æ›´æ–°è¯´æ˜

æœ¬æ–‡è®°å½•äº†``Edge10``å¤§æ¨¡å‹å·¥å…·é“¾``TyLLM``çš„å˜æ›´æƒ…å†µã€‚

**20250305/v0.0.2**

- ğŸš€æ–°å¢æ”¯æŒ``Qwen2.5-VL-7B``æ¨¡å‹``4Die/1Die``ç¼–è¯‘


## æ•´ä½“ä»‹ç»

``TyLLM``æ˜¯äº‘å¤©åŠ±é£æ¨å‡ºçš„å¤§æ¨¡å‹å·¥å…·é“¾ï¼Œå¯å¸®åŠ©ç”¨æˆ·å°†å¤§æ¨¡å‹ç¼–è¯‘ä¸º``Edge10``ç³»åˆ—èŠ¯ç‰‡ä¸Šæ‰§è¡Œçš„æ¨¡å‹ã€‚é™¤``TyLLM``è¿˜åŒ…æ‹¬``AutoAWQ``å’Œ``TyTVM``ï¼Œ``AutoAWQ``ä¸ºäº‘å¤©åŸºäºç¤¾åŒºå¼€æºç‰ˆæœ¬è‡ªè¡Œç»´æŠ¤çš„é‡åŒ–æ¨¡å—ï¼Œç›®çš„æ˜¯æœ€å¤§åŒ–æ¨¡å‹é‡åŒ–ï¼Œåœ¨``Edge10``ç³»åˆ—èŠ¯ç‰‡ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼›``TyLLM``ä¸ºäº‘å¤©åŸºäº``TyTVM``å·¥å…·é“¾é’ˆå¯¹å¤§æ¨¡å‹å¢é‡å¼€å‘çš„å·¥å…·ï¼Œä¸»è¦åŸºäº``PyTorch``å’Œ``vLLM``å¯¹å¤§æ¨¡å‹åšä¸“å±å’Œå®šåˆ¶ä¼˜åŒ–ï¼›``TyTVM``ä¸ºäº‘å¤©æ¨¡å‹è½¬æ¢ã€é‡åŒ–ã€ä»¿çœŸã€ç¼–è¯‘å·¥å…·é“¾ï¼Œä¸»è¦è´Ÿè´£å°†æ¨¡å‹ç¼–è¯‘ä¸ºèŠ¯ç‰‡æ‰§è¡Œçš„æ¨¡å‹ã€‚

### æ•´ä½“æ¶æ„å¦‚å›¾ï¼š

<div style="text-align:center;">
  <img src="./assets/whiteboard_exported_image.png" alt="æ¶æ„å›¾" style="width:100%; height:auto;" />
</div>

### æ¨¡å‹åˆ—è¡¨

å·²ç»æ”¯æŒçš„æ¨¡å‹å¦‚ä¸‹ï¼ˆåŒ…æ‹¬ä¸é™äºï¼‰ï¼š

| Model     | Support     |
| :-------- | :---------: |
| Qwen/Qwen2.5-VL-7B                            | âœ…     |
| Qwen/Qwen2-VL-7B                              | âœ…     |
| Qwen/Qwen2-7B                                 | âœ…     |
| Qwen/Qwen1.5-1.8B                             | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B     | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-7B       | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B      | âœ…     |
| Llama3-8B                                     | âœ…     |


## å¿«é€Ÿå¼€å§‹

æœ¬èŠ‚ä»‹ç»ä½¿ç”¨``TyLLMå·¥``å…·é“¾å‰çš„å¼€å‘ç¯å¢ƒå‡†å¤‡å·¥ä½œã€‚``TyLLM``ä½¿ç”¨``Docker``å®¹å™¨è¿›è¡Œå·¥å…·é“¾é›†æˆï¼Œç”¨æˆ·å¯é€šè¿‡``Docker``åŠ è½½``TyLLM``é•œåƒæ–‡ä»¶ï¼Œç„¶åè¿›è¡Œæ¨¡å‹é‡åŒ–ã€ç¼–è¯‘ã€è¯„ä¼°(æœªæ¥)ç­‰å·¥ä½œï¼Œå› æ­¤å¼€å‘ç¯å¢ƒå‡†å¤‡é˜¶æ®µéœ€è¦æ­£ç¡®å®‰è£…``Docker``ç¯å¢ƒï¼ŒåŒæ—¶ç›®å‰éœ€è¦é‡åŒ–é˜¶æ®µéœ€è¦``GPU``æ¥åŠ é€Ÿï¼Œä»¥åŠå¤šæ¨¡æ€æ¨¡å‹çš„ç¼–è¯‘ä¾èµ–``vLLM``æ¡†æ¶æ¥æ¨ç†ï¼Œå› æ­¤æš‚æ—¶éœ€è¦``GPU``ã€‚

### ç¯å¢ƒå‡†å¤‡

- **Nvidia GPU**
- **Nvidia Container Toolkit**
- **Docker>19.03**

#### å®‰è£…Nvidia GPU é©±åŠ¨

```shell
sudo apt install nvidia-driver-530 # é©±åŠ¨ç‰ˆæœ¬å°½é‡é€‰æ‹©æœ€é«˜
# å®‰è£…å®Œæˆåï¼Œæ‰§è¡Œnvidia-smiå‘½ä»¤æ˜¾ç¤ºå¦‚ä¸‹ï¼Œè¡¨ç¤ºå®‰è£…æˆåŠŸã€‚
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.179                Driver Version: 535.179      CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:84:00.0 Off |                  N/A |
| 30%   27C    P8              23W / 350W |      3MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

#### å®‰è£…Docker

```shell
sudo apt install docker.io
sudo docker -v
# Docker version 20.10.21, build 20.10.21-0ubuntu1~20.04.2
```

#### å®‰è£…Nvidia Container Toolkit

æ·»åŠ åŒ…ä»“åº“å’Œ``GPG key``:
```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
    && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
             sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
             sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

æ›´æ–°æºï¼Œå®‰è£…nvidia-container-toolkit

```shell
sudo apt update
sudo apt install nvidia-container-toolkit
```

#### å®‰è£…TyLLMå·¥å…·é“¾

ä»¥ç³»ç»Ÿç‰ˆæœ¬``Ubuntu 20.04``ã€å·¥å…·é“¾``tyllm_${version}.zip``ä¸ºä¾‹è¯´æ˜``TyLLM``å·¥å…·é“¾çš„å®‰è£…æ–¹æ³•ã€‚å®é™…æ“ä½œæ—¶ï¼Œè¯·åŠ¡å¿…å°†``${version}``æ›¿æ¢ä¸ºå®é™…å¯¹åº”çš„å·¥å…·é“¾ç‰ˆæœ¬å·ï¼Œæ¯”å¦‚``tyllm_v0.0.2.zip``

å·¥å…·é“¾è·å–é€”å¾„

- äº‘å¤© docker hub

    ```shell
    sudo docker login 113.100.143.90:8091 -u custom -p DE@sz_intellif_2021
    sudo docker pull 113.100.143.90:8091/dengine/tyllm:v0.0.2
    ```

> **æ³¨æ„**ï¼Œéœ€è¦å°†``113.100.143.90:8091``åŠ å…¥``/etc/docker/daemon.json``ä¸­çš„``insecure-registries``å­—æ®µä¸­ï¼Œå¦‚ä¸‹ï¼š
> 
> ```json
> {     
>      "insecure-registries": ["113.100.143.90:8091"]
> }
>  ```
> ä¿®æ”¹åï¼Œé‡å¯``docker``ç”Ÿæ•ˆï¼Œ``sudo systemctl restart docker``

#### è½½å…¥Docker Image

```shell
sudo docker load -i tyllm_v0.0.2.zip
# è½½å…¥æˆåŠŸåï¼ŒæŸ¥çœ‹é•œåƒ
sudo docker images
# è¾“å‡ºç»“æœå¦‚ä¸‹
REPOSITORY                              TAG     IMAGE ID       CREATED         SIZE
113.100.143.90:8091/dengine/tyllm       v0.0.2  a4a57c8af885   29 hours ago    10.4GB
```

#### å¯åŠ¨å·¥å…·é“¾é•œåƒ

ä»¥ä¸‹å‘½ä»¤åˆ›å»ºå®¹å™¨ï¼Œå…¶ä¸­``${your_data_dir}``è¡¨ç¤ºå®¿ä¸»æœºä¸­ç”¨æˆ·æ•°æ®ç›®å½•ã€‚
```shell
docker run --gpus all -v ${your_data_dir}:/data -it 113.100.143.90:8091/dengine/tyllm:v0.0.2 bash
```

### æ¨¡å‹é‡åŒ–

äº‘å¤©çš„å¤§æ¨¡å‹é‡åŒ–æ˜¯é€šè¿‡``AutoAWQ``å·¥å…·å®Œæˆï¼Œè¯¥å·¥å…·æ˜¯äº‘å¤©åŸºäºç¤¾åŒºç‰ˆæœ¬å¼€å‘ç»´æŠ¤ï¼Œå…¶ç”¨æ³•ä¸ç¤¾åŒºç‰ˆæœ¬åŸºæœ¬ä¸€è‡´ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = "Qwen/Qwen2.5-VL-7B-Instruct"
quant_path = "./Qwen2.5-VL-7B-Instruct-AWQ-INT4"

model = AutoAWQForCausalLM.from_pretrained(model_path, torch_dtype="float16", cache_dir="./")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir="./")

quant_config = {"zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM", "lm_head_enable": True}
model.quantize(tokenizer, quant_config=quant_config)
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)
```

> **æ³¨æ„**
> - å…¶ä¸­ï¼Œ``zero_point``ã€``q_group_size``ã€``w_bit``ã€``version``ä¸€èˆ¬å›ºå®šä¸ºç¤ºä¾‹ä¸­çš„å‚æ•°ï¼Œä»…``lm_head_enable``å‚æ•°éœ€è¦åº”æƒ…å†µè°ƒæ•´ï¼Œå¤§å¤šæƒ…å†µä¸‹ä¸º``True``ï¼Œå°‘éƒ¨åˆ†æ¨¡å‹å¯èƒ½ä¼šå½±å“èŠ¯ç‰‡ä¸Šæ¨ç†æ€§èƒ½ï¼›
> - åŠ è½½åŸå§‹æ¨¡å‹çš„æ—¶å€™éœ€è¦æ˜ç¡®æŒ‡å®š``torch_dtype="float16"``ï¼Œå› ä¸ºé»˜è®¤æ˜¯``"bfloat16"``ï¼Œå¦‚æœå­˜åœ¨éƒ¨åˆ†æœªé‡åŒ–çš„``module``ï¼Œå…¶``weight``ä»ç„¶æ˜¯``bfloat16``ï¼Œäº‘å¤©èŠ¯ç‰‡ä¸æ”¯æŒï¼›
> - æ¨¡å‹ä¸‹è½½å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼›
> - ä¹Ÿå¯ä½¿ç”¨ç¤¾åŒºç‰ˆæœ¬è¿›è¡Œé‡åŒ–ï¼Œä½†éœ€è¦ç§»é™¤``lm_head_enable``ï¼›


### æ¨¡å‹ç¼–è¯‘

æœ¬èŠ‚ä¸»è¦ä»‹ç»é‡åŒ–å¤§æ¨¡å‹çš„ç¼–è¯‘ï¼Œç›®å‰åˆ†ä¸ºè¯­è¨€å¤§æ¨¡å‹å’Œè§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼Œç¼–è¯‘æ–¹å¼ç¨æœ‰ä¸åŒï¼Œä»¥ä¸‹é€šè¿‡è¯¦ç»†ç¤ºä¾‹ä»£ç è¯´æ˜ï¼š

#### è¯­è¨€å¤§æ¨¡å‹

ä»¥``Qwen1.5-1.8B``ä¸ºä¾‹ï¼š

```python
from tyllm.build_util import build_and_compile_llm

quant_path = "./Qwen1.5-1.8B-AWQ-INT4"
aot_path = f"{quant_path}-AOT"

# é¢„å¡«å……åºåˆ—é•¿åº¦
prefill_seq_len = 8
# æœ€å¤§KVé”®å€¼å¯¹æ•°ï¼Œæ§åˆ¶æ¨¡å‹æ¨ç†æœŸé—´ä¸Šä¸‹æ–‡é•¿åº¦
max_kv_cache_size = 4096
# æŒ‡å®šå¤šdieç¼–è¯‘ï¼Œå¤šdieå¹¶è¡Œè®¡ç®—
die_num = 4
# æ˜¯å¦å°†embeddingæ“ä½œä½œä¸ºè¾“å…¥ï¼Œé»˜è®¤Falseï¼›å¦‚æœTrueï¼Œembeddingè®¡ç®—å°†è¢«offloadåˆ°cpu
embedding_as_input = False

build_and_compile_llm(
    model_path=quant_path,
    artifacts_path=f"{aot_path}/{die_num}die",
    max_kv_cache_size=max_kv_cache_size,
    seq_len_list=[1, prefill_seq_len],
    dev_count=die_num,
    embedding_as_input=embedding_as_input,
)
```

**å‚æ•°è¯´æ˜**ï¼š

- **model_path(str)** ``huggingface``æ¨¡å‹å’Œé…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼›
- **max_kv_cache_size(int, optional)** ``kv``ç¼“å­˜çš„æœ€å¤§å®¹é‡ï¼Œé»˜è®¤ä¸º``4096``ï¼›
- **seq_len_list(list of int, optional)** ç”¨äºæ„å»ºå’Œç¼–è¯‘æ¨¡å‹çš„åºåˆ—é•¿åº¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º``[1, 8]``ï¼›
- **dev_count(int, optional)** ç”¨äºè¿è¡Œå·²ç¼–è¯‘æ¨¡å‹çš„è®¾å¤‡æ•°é‡ï¼ˆå¦‚ NNP è®¾å¤‡ï¼‰ï¼Œé»˜è®¤ä¸º``1``ï¼›
- **artifacts_path(str, optional)** ä¿å­˜æ¨¡å‹ç¼–è¯‘äº§ç‰©ï¼ˆå¦‚æƒé‡ã€åµŒå…¥å±‚ç­‰ï¼‰çš„ç›®å½•è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨``model_path``ä½œä¸ºé»˜è®¤è·¯å¾„ï¼›
- **embedding_as_input(bool, optional)** å¦‚æœä¸º``True``ï¼Œå°†æå–åµŒå…¥å±‚å¹¶å•ç‹¬ä¿å­˜ä¸º``NumPy``æ•°ç»„ï¼Œé»˜è®¤ä¸º``False``ï¼›

**ç¼–è¯‘åäº§ç‰©ç›®å½•**ï¼š

```shell
Qwen1.5-1.8B-AWQ-INT4
â”œâ”€â”€ added_tokens.json
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ merges.txt
â”œâ”€â”€ model.safetensors
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ vocab.json
```

#### è§†è§‰è¯­è¨€å¤§æ¨¡å‹

åŸºäº``vllm``çš„``Qwen2.5-VL``ç¤ºä¾‹ï¼š

```python
import logging
import numpy as np
import torch
from PIL import Image
from vllm import LLM, SamplingParams
from vllm.config import ModelConfig
from tyllm.vllm_ext.edgex_executor import EdgeXExecutor
from tyllm import torch_edgex

os.environ["TOKENIZERS_PARALLELISM"] = "true"
logging.getLogger("vllm").setLevel(logging.WARNING)

model_dir = "./Qwen2.5-VL-7B-Instruct-AWQ-INT4"

# æŒ‡å®šå¤šdieç¼–è¯‘ï¼Œå¤šdieå¹¶è¡Œè®¡ç®—
num_die = 4
# é¢„å¡«å……åºåˆ—é•¿åº¦
prefill_lens = 96
# æ¨¡å‹åœ¨å•æ¬¡æ¨ç†ä¸­èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥ + è¾“å‡ºçš„æ€» token æ•°é‡ï¼‰ã€‚
max_model_len = 4096
# æ¨¡å‹è§†è§‰éƒ¨åˆ†è¾“å…¥size
input_size = (540, 960, 3)  #(H, W, 3), (1080, 1920, 3), (720, 1280, 3), (540, 960, 3), (448, 768, 3)  (360, 640, 3)

torch_edgex.edgex_module.set_trace_only_mode(True)
torch_edgex.set_device_mode("exec_mode", "AOT")
torch_edgex.set_device_mode("prefill_lens", [1, prefill_lens])
# è®¾ç½®è¾“å‡ºç›®å½•
torch_edgex.set_device_mode("AOT_DIR", f"./Qwen2.5-VL-7B-Instruct-AWQ-INT4-AOT_{input_size[1]}x{input_size[0]}_{max_model_len}")
# è®¾ç½®å¤šdieåºåˆ—ï¼Œå…¶ä¸­é¦–ä½è¡¨ç¤ºdie0è¡¨ç¤ºä¸»die
torch_edgex.set_device_mode("die_remap", [0, 1, 2, 3])

ModelConfig.verify_with_parallel_config = lambda a, b: True
torch._dynamo.reset()

def main():
    modality = "image"
    random_image = np.random.randint(0, 256, input_size, dtype=np.uint8)
    data = Image.fromarray(random_image)
    question = "è¯·æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹"

    llm = LLM(
        model=model_dir,
        max_model_len=max_model_len,
        tensor_parallel_size=num_die,  # die
        # ä»¥ä¸‹å‚æ•°ä¸ç¡¬ä»¶æ— å…³
        max_num_seqs=5,
        # Note - mm_processor_kwargs can also be passed to generate/chat calls
        mm_processor_kwargs={
            "min_pixels": 256 * 28 * 28,
            "max_pixels": 1280 * 28 * 28,
        },
        disable_mm_preprocessor_cache=True,
        trust_remote_code=True,
        disable_async_output_proc=True,
        distributed_executor_backend=EdgeXExecutor,
        worker_cls="tyllm.vllm_ext.edgex_executor.EdgeXWorker",
    )

    if modality == "image":
        placeholder = "<|image_pad|>"
    elif modality == "video":
        placeholder = "<|video_pad|>"

    prompt = (
        "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
        f"<|im_start|>user\n<|vision_start|>{placeholder}<|vision_end|>"
        f"{question}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    # Single inference
    inputs = {
        "prompt": prompt,
        "multi_modal_data": {modality: data},
    }

    _ = llm.generate(inputs, use_tqdm=False)

if __name__ == "__main__":
    main()
```

ç¼–è¯‘åäº§ç‰©ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```shell
Qwen2.5-VL-7B-Instruct-AWQ-INT4-AOT_960x540_4096/
â””â”€â”€ 4die
    â”œâ”€â”€ batch_1
    â”‚Â Â  â”œâ”€â”€ common_die0.params
    â”‚Â Â  â”œâ”€â”€ common_die1.params
    â”‚Â Â  â”œâ”€â”€ common_die2.params
    â”‚Â Â  â”œâ”€â”€ common_die3.params
    â”‚Â Â  â”œâ”€â”€ seqlen_1
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die1.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die1.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die2.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die2.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die3.params
    â”‚Â Â  â”‚Â Â  â””â”€â”€ llm_die3.so
    â”‚Â Â  â””â”€â”€ seqlen_96
    â”‚Â Â      â”œâ”€â”€ llm_die0.params
    â”‚Â Â      â”œâ”€â”€ llm_die0.so
    â”‚Â Â      â”œâ”€â”€ llm_die1.params
    â”‚Â Â      â”œâ”€â”€ llm_die1.so
    â”‚Â Â      â”œâ”€â”€ llm_die2.params
    â”‚Â Â      â”œâ”€â”€ llm_die2.so
    â”‚Â Â      â”œâ”€â”€ llm_die3.params
    â”‚Â Â      â””â”€â”€ llm_die3.so
    â”œâ”€â”€ buffer_config.json
    â”œâ”€â”€ config.json
    â”œâ”€â”€ embedding.params
    â”œâ”€â”€ empty.bin
    â”œâ”€â”€ mrope
    â”‚Â Â  â”œâ”€â”€ 3_4096_[int32].onnx
    â”‚Â Â  â””â”€â”€ 3_4096_[int32].so
    â””â”€â”€ visual
        â”œâ”€â”€ 2584_1176_[float16].onnx
        â”œâ”€â”€ 2584_1176_[float16].so
        â””â”€â”€ 2584_1176_[float16]_preset_kwargs.pt
```


## å¸¸è§é—®é¢˜

è‹¥åœ¨ä½¿ç”¨äº§å“è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒæ­¤æ–‡æ¡£ã€‚

### ç¼–è¯‘é˜¶æ®µå‡ºç°segmentation fault(core dump)

**é—®é¢˜æè¿°**ï¼š

ç¼–è¯‘è¿‡ç¨‹ä¸­å¦‚æœå‡ºç°æ®µé”™è¯¯ï¼Œå¯èƒ½æ˜¯pyarrowåŒ…ç‰ˆæœ¬é—®é¢˜

**è§£å†³æ–¹æ³•**ï¼š

å°†``pyarrow``åŒ…é™çº§è‡³``16.0.0``ï¼Œ
```shell
pip install pyarrow==16.0.0
```

### <é—®é¢˜äºŒ>

è‹¥æœ‰å¿…è¦ï¼Œåˆ™åœ¨å¼€å¤´å…·ä½“æè¿°è¯¥é—®é¢˜ï¼Œæä¾›æ›´å¤šç»†èŠ‚ä¿¡æ¯ã€‚

**é—®é¢˜æè¿°**ï¼š

**è§£å†³æ–¹æ³•**ï¼š


### <é—®é¢˜ N>

è‹¥æœ‰å¿…è¦ï¼Œåˆ™åœ¨å¼€å¤´å…·ä½“æè¿°è¯¥é—®é¢˜ï¼Œæä¾›æ›´å¤šç»†èŠ‚ä¿¡æ¯ã€‚

**é—®é¢˜æè¿°**ï¼š

**è§£å†³æ–¹æ³•**ï¼š

